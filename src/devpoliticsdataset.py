# -*- coding: utf-8 -*-
"""DEVPoliticsDataset.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-1CATzPCSLI1-oDo9SvNGO4dLazgSJNn
"""

# Commented out IPython magic to ensure Python compatibility.
# %tensorflow_version 1.x
!pip install -U torchviz
!pip install ekphrasis torch transformers emoji swifter
import torch
from transformers import AutoModel, AutoTokenizer 
from torch.utils.data import Dataset, DataLoader
from sklearn.preprocessing import MultiLabelBinarizer
!pip install ekphrasis sentence-transformers
!pip install -U bert-serving-server  # server
!pip install -U bert-serving-client  # client, independent of `bert-serving-server`
from torch import nn
import torch.optim as optim
import numpy as np
import torch.nn.functional as F
import emoji
from google.colab import drive
from collections import Counter
import os
import matplotlib.pyplot as plt
import pandas as pd
import swifter
from bert_serving.client import BertClient
from nltk.tokenize import TweetTokenizer
from ekphrasis.classes.preprocessor import TextPreProcessor
from ekphrasis.classes.tokenizer import SocialTokenizer
from ekphrasis.dicts.emoticons import emoticons
!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip
!unzip -o uncased_L-12_H-768_A-12.zip
!nohup bert-serving-start -model_dir=./uncased_L-12_H-768_A-12 -num_worker=4 > out.file 2>&1 & 
bertClient = BertClient(check_length=False)
#bertTrans = SentenceTransformer('paraphrase-MiniLM-L6-v2')
device = "cuda" if torch.cuda.is_available() else "cpu"
#device = "cpu"
MOST_COMMON = 128
tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased", normalization=True, use_fast=False)
bertweet = AutoModel.from_pretrained("bert-base-uncased")



# Commented out IPython magic to ensure Python compatibility.
def _download_recourses():
  drive.mount('/content/gdrive')
  os.environ['KAGGLE_CONFIG_DIR'] = "/content/gdrive/My Drive"
#   %cd /content/gdrive/My Drive/Kaggle
  !yes | kaggle datasets download -d manchunhui/us-election-2020-tweets
  !unzip -o us-election-2020-tweets.zip
  !cat hashtag_donaldtrump.csv | tail -n 10

def _bulk_load_from_files():
    trump_unparsed_data = pd.read_csv('./hashtag_donaldtrump.csv', lineterminator='\n',
                                usecols=['user_screen_name', 'likes', 'tweet'])
    biden_unparsed_data = pd.read_csv('./hashtag_joebiden.csv', lineterminator='\n',
                                usecols=['user_screen_name', 'likes', 'tweet'])
    unparsed_data = trump_unparsed_data.append(biden_unparsed_data)
    data = unparsed_data.iloc[[index for index, value in unparsed_data.tweet.str.contains('#').iteritems() if value]]
    data =data.reset_index(drop=True).sort_values('user_screen_name')#.sample(n=50000, random_state=42)
    return data

def _extract_tags(tweets: pd.Series):
  return tweets.apply(lambda lstOfTokens: [token for token in lstOfTokens if token.startswith('#')])

def _tokenize(row_tweets: pd.Series):
  print("---TOKENIZING TWEETS NOW---")
  text_processor = TextPreProcessor(
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
    annotate={"hashtag",# "allcaps", 
              "elongated", "repeated",
        'emphasis', 'censored'},
    fix_html=True,  # fix HTML tokens
    segmenter="twitter", 
    corrector="twitter", 
    #unpack_hashtags=True,  # perform word segmentation on hashtags
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=False,  # spell correction for elongated words
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    dicts=[emoticons]
  )
  return row_tweets.apply(text_processor.pre_process_doc)

def _filter_bad_tweets(data: pd.DataFrame):
  data = data[data['tweet'].apply(lambda tweet: 4 < len(tweet) < 50)].reset_index(drop=True)
  data = data[data['likes'].apply(lambda likes: likes > 10)].reset_index(drop=True)
  return data[data['tags'].apply(lambda lst : len(lst) > 0)].reset_index(drop=True)

def _encode_with_bert(tweets: pd.Series):
  return tweets.apply(lambda tweet: bertClient.encode([tweet], is_tokenized=True))

def _bertweet_encode(tweets: pd.Series):
  tokenized = tweets.apply(lambda x: tokenizer.encode(x, add_special_tokens=True, max_length=128))
  max_len = 0
  for i in tokenized.values:
      if len(i) > max_len:
          max_len = len(i)
  padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])
  input_ids = torch.tensor(np.array(padded))
  with torch.no_grad():
    last_hidden_states = bertweet(input_ids)
  return last_hidden_states[0][:,0,:].numpy()

def _one_hot_encode_tags(tags: pd.Series):
    all_tags = []
    tags.apply(all_tags.extend)
    top_tags, top_counts = zip(*Counter(all_tags).most_common(MOST_COMMON))
    recognized_tags = set(top_tags)
    tags = tags.map(lambda prev_tags: [tag if tag in recognized_tags else "OTHER" for tag in prev_tags])
    recognized_tags.add('OTHER')
    mlb = MultiLabelBinarizer()
    mlb.fit([recognized_tags])
    return tags.apply(lambda lst: mlb.transform([lst]))

def _normalize_likes_by_author(df: pd.DataFrame):
  likes_author = df[['user_screen_name', 'likes']]
  a = likes_author.groupby('user_screen_name').transform(lambda x: x / x.max()).fillna(0)
  a[a == float('inf')] = 0.5
  return a

_download_recourses()

data = _bulk_load_from_files()
print("---DATA EXTRACTED FROM CSV FILES---")
data.head()

data = data[1::2].reset_index(drop=True)

data = data[data['likes'].apply(lambda likes: likes > 10)].reset_index(drop=True)

data['row'] = data['tweet']
# data['row'] = data.row.apply(emoji.demojize)

MOST_COMMON = 64

data.shape

MOST_COMMON = 64

data['tweet'] = data['tweet'].apply(emoji.demojize)
data['tweet'] = _tokenize(data['tweet'])
print("---TWEETS TOKENIZED---")
data['tweet'].head(20)

data.shape

data = data.sample(15000, random_state=42).reset_index(drop=True)

encoded_with_bertweet = pd.Series()
for i in range(1000, len(data) + 1, 1000):
  encoded_with_bertweet = pd.concat([encoded_with_bertweet, pd.Series(list(_bertweet_encode(data.row[i-1000:i])))],ignore_index=True)
  print(i)

assert(encoded_with_bertweet.shape[0] == 15000)

data['tags'] = _extract_tags(data['tweet'])
data = data.reset_index(drop=True)
print("---TAGS EXTRACTED FROM ROW TWEETS---")
data['tags'].head(20)



data = _filter_bad_tweets(data)
data = data.reset_index(drop=True)
print("---TWEETS FILTERED FOR WORDS---")



data['encoded_tweets'] = _encode_with_bert(data['tweet'])
print("---TWEETS ENCODED WITH BERT---")

data['encoded_tweets'] = encoded_with_bertweet

data['encoded_tags'] = _one_hot_encode_tags(data['tags'])
print("---TAGS ENCODED---")

data['encoded_tags'][0].sum()

data['normed_likes'] = _normalize_likes_by_author(data)



class PoliticsDataset(Dataset):
  def __init__(self, dt, applyFunc=None):
    self.data = dt
    self.applyFunc = applyFunc
    self.tags_vector_dimension = self.data['encoded_tags'][0].shape[1]

    self.data['encoded_tags'] = self.data['encoded_tags'].apply(lambda v: toTensor(v).view(-1).to(device))
    self.data['encoded_tweets'] = self.data['encoded_tweets'].apply(lambda v: toTensor(v).view(-1).to(device))
    self.data['normed_likes'] = self.data['normed_likes'].apply(lambda v: toTensor(v).view(-1).to(device))
    print("---INIT FINISHED---")
  def __len__(self):
    return self.data.shape[0]

  def __getitem__(self, index):
    input = (self.data['encoded_tweets'][index], self.data['encoded_tags'][index])
    target = self.data['normed_likes'][index]
    return (input, target)
    #input = (self.applyFunc(self.data['encoded_tweets'][index]).view(-1).to(device), self.applyFunc(self.data['encoded_tags'][index]).view(-1).to(device))
    #target = self.applyFunc(self.data['normed_likes'][index]).view(-1).to(device)

def toTensor(x: np.float64):
  return torch.from_numpy(np.asarray(x)).float()



from sklearn.model_selection import train_test_split
data_train, data_val = train_test_split(data, test_size=0.2, random_state=42)

dataset_train = PoliticsDataset(dt=data_train.reset_index(drop=True), applyFunc=toTensor)
dataset_val   = PoliticsDataset(dt=data_val.reset_index(drop=True), applyFunc=toTensor)

dataloader = DataLoader(dataset=dataset_train, batch_size=64, shuffle=True)
dataloader_val = DataLoader(dataset=dataset_val, batch_size = len(dataset_val), shuffle=True)



class LinearBertForSentences(nn.Module):

    def __init__(self):
      super(LinearBertForSentences, self).__init__()
      self.sentenceLayers = nn.Sequential(
        nn.Linear(768, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Linear(128, 64),
        nn.ReLU()
      )
      self.tagsLayers = nn.Sequential(
          nn.Linear(MOST_COMMON + 1, 256),
          nn.ReLU(),
          nn.Linear(256, 64),
          nn.ReLU()
      )
      self.finalLayers = nn.Sequential(
          nn.Linear(128, 64),
          nn.ReLU(),
          nn.Linear(64, 1)
      )
    def forward(self, batch: list):
        assert(len(batch) == 2)
        sentence, tags = batch
        sentence = self.sentenceLayers(sentence)
        tags = self.tagsLayers(tags)
        #catted = torch.cat([sentence, tags, followers], dim=0)
        return self.finalLayers(torch.cat([sentence, tags], dim=1))

data.encoded_tags[0]

running_loss = 0.0
model = LinearBertForSentences()

if device is not None and device != 'cpu':
  model = model.cuda()
lossFunc = nn.MSELoss()
optimizer = optim.Adam(model.parameters())
batch_processed, losses, eval_losses = [], [], []
#Evaluation before any learning (kinda trash expected, cos weights are random values from memory)
model.eval()
train_item = next(iter(dataloader_val))
with torch.no_grad():
  ls = lossFunc(model(train_item[0]), train_item[1])
  print(f'EPOCH: {0}\teval_loss: {ls}')
  eval_losses.append(ls)
model.train()


for ep in range(32000):
  for i, dataitem in enumerate(dataloader, 0):
    inputs, labels = dataitem
    optimizer.zero_grad()
    outputs = model(inputs)
    loss = lossFunc(outputs, labels)
    loss.backward()
    optimizer.step()
    running_loss += loss.item()
    if i % 10 == 9:
        batch_processed.append(i + ep*len(dataloader))
        losses.append(running_loss)
        running_loss = 0.0
  model.eval()
  with torch.no_grad():
    ls = lossFunc(model(train_item[0]), train_item[1])
    print(f'EPOCH: {ep + 1}\teval_loss: {ls}')
    eval_losses.append(ls)
  model.train()

#plt.bar(batch_processed, losses, label="Unlogged", color='r')

#plt.fill_between(range(len(eval_losses)), eval_losses, color='r')
plt.xlabel('Amount of epoch processed')
plt.ylabel('Loss value on validation data')
plt.plot(range(len(losses)), losses, color='r')
plt.show()

torch.save(model.state_dict(), './model.bin')

all_tags = []
dataloader.dataset.data['tags'].apply(all_tags.extend)
top_tags, top_counts = zip(*Counter(all_tags).most_common(MOST_COMMON))
recognized_tags = set(top_tags)
recognized_tags
recognized_tags.add('OTHER')
mlb = MultiLabelBinarizer()
mlb.fit([recognized_tags])
toTensor(mlb.transform([[next(iter(recognized_tags))]]))

"while True:"
sent = input("Input sentence PLSLSLL:")
text_processor = TextPreProcessor(
    normalize=['url', 'email', 'percent', 'money', 'phone', 'user',
        'time', 'url', 'date', 'number'],
    annotate={"hashtag",# "allcaps", 
              "elongated", "repeated",
        'emphasis', 'censored'},
    fix_html=True,  # fix HTML tokens
    segmenter="twitter", 
    corrector="twitter", 
    #unpack_hashtags=True,  # perform word segmentation on hashtags
    unpack_contractions=True,  # Unpack contractions (can't -> can not)
    spell_correct_elong=False,  # spell correction for elongated words
    tokenizer=SocialTokenizer(lowercase=True).tokenize,
    dicts=[emoticons]
  )
sent =  toTensor(_bertweet_encode(pd.Series(sent))).to(device)
model.eval()
res = {}
items = []
with torch.no_grad():
  for tag in recognized_tags:
    nowRes = model([sent, toTensor(mlb.transform([[tag]])).to(device)])
    res[nowRes.item()] = tag
    items.append(nowRes.item())
    items.sort(reverse=True)
[(k, res[k]) for k in items]
#dict(sorted(res.items(), key=lambda item: item[1]))

from torchviz import make_dot, make_dot_from_trace
graph = make_dot(model(next(iter(dataloader))[0]), params=dict(model.named_parameters()))
graph.format = 'png'
graph.render()
graph.save()

model(next(iter(dataloader))[0])